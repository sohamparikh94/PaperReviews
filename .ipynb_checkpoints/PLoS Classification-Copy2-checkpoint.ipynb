{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from classifier_utils import ClassifierUtils\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp_light = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "alphabet = string.ascii_lowercase\n",
    "clf_utils = ClassifierUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/review_decisions.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_revision = dict()\n",
    "for doc_id in data:\n",
    "    if('revisions') in data[doc_id]:\n",
    "        for revision in data[doc_id]['revisions']:\n",
    "            if(revision not in data_by_revision):\n",
    "                data_by_revision[revision] = dict()\n",
    "            if('reviews' in data[doc_id]['revisions'][revision]):\n",
    "                for review in data[doc_id]['revisions'][revision]['reviews']:\n",
    "                    review_obj = data[doc_id]['revisions'][revision]['reviews'][review]\n",
    "                    if('text' in review_obj and 'decision' in review_obj):\n",
    "                        if(len(review_obj['text'].strip()) > 0 and len(review_obj['decision']) > 0):\n",
    "                            if(review_obj['decision'] not in data_by_revision[revision]):\n",
    "                                data_by_revision[revision][review_obj['decision']] = list()\n",
    "                            data_by_revision[revision][review_obj['decision']].append(review_obj['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data_by_revision['0']['Reject'] + data_by_revision['0']['Accept'] + data_by_revision['0']['Major Revision'] + data_by_revision['0']['Minor Revision']\n",
    "labels = [0]*len(data_by_revision['0']['Reject']) + [1]*len(data_by_revision['0']['Accept']) + [2]*len(data_by_revision['0']['Major Revision']) + [3]*len(data_by_revision['0']['Minor Revision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_nb = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels)\n",
    "    all_metrics_nb.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_features = clf_utils.get_nb_features(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[0][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[1][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[2][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[3][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_lr_ovr = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR')\n",
    "    all_metrics_lr_ovr.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are features from One vs Rest\n",
    "\"\"\"\n",
    "ordered_features = clf_utils.get_lr_features(documents, labels, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[0][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[1][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[2][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in ordered_features[3][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_lr_mn = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR', multi_class='multinomial')\n",
    "    all_metrics_lr_mn.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are features from Multinomial Logistic Regression\n",
    "\"\"\"\n",
    "ordered_features = clf_utils.get_lr_features(documents, labels, multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[0][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[1][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[2][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[3][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = dict()\n",
    "wordcloud['Reject'] = WordCloud().generate(' '.join(data_by_revision['0']['Reject']))\n",
    "wordcloud['Accept'] = WordCloud().generate(' '.join(data_by_revision['0']['Accept']))\n",
    "wordcloud['Major Revision'] = WordCloud().generate(' '.join(data_by_revision['0']['Major Revision']))\n",
    "wordcloud['Minor Revision'] = WordCloud().generate(' '.join(data_by_revision['0']['Minor Revision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Reject'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Accept'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Major Revision'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Minor Revision'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=clf_utils.tokenizer)\n",
    "count_vectorizer.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_features = count_vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boolean_reject = np.array(labels) == np.array([0]*len(labels))\n",
    "int_reject = np.array([int(value) for value in boolean_reject])\n",
    "boolean_accept = np.array(labels) == np.array([1]*len(labels))\n",
    "int_accept = np.array([int(value) for value in boolean_accept])\n",
    "boolean_major = np.array(labels) == np.array([2]*len(labels))\n",
    "int_major = np.array([int(value) for value in boolean_major])\n",
    "boolean_minor = np.array(labels) == np.array([3]*len(labels))\n",
    "int_minor = np.array([int(value) for value in boolean_minor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()\n",
    "correlations_reject = [np.corrcoef(document_features[:,idx], int_reject)[0][1] for idx in range(len(feature_names))]\n",
    "correlations_accept = [np.corrcoef(document_features[:,idx], int_accept)[0][1] for idx in range(len(feature_names))]\n",
    "correlations_major = [np.corrcoef(document_features[:,idx], int_major)[0][1] for idx in range(len(feature_names))]\n",
    "correlations_minor = [np.corrcoef(document_features[:,idx], int_minor)[0][1] for idx in range(len(feature_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_names_reject = [feature_names[idx] for idx in np.argsort(correlations_reject)][::-1]\n",
    "sorted_feature_names_accept = [feature_names[idx] for idx in np.argsort(correlations_accept)][::-1]\n",
    "sorted_feature_names_major = [feature_names[idx] for idx in np.argsort(correlations_major)][::-1]\n",
    "sorted_feature_names_minor = [feature_names[idx] for idx in np.argsort(correlations_minor)][::-1]\n",
    "sorted_correlations_reject = np.sort(correlations_reject)[::-1]\n",
    "sorted_correlations_accept = np.sort(correlations_accept)[::-1]\n",
    "sorted_correlations_major = np.sort(correlations_major)[::-1]\n",
    "sorted_correlations_minor = np.sort(correlations_minor)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_reject[:20]):\n",
    "    print(feature, sorted_correlations_reject[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_accept[:20]):\n",
    "    print(feature, sorted_correlations_accept[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_major[:20]):\n",
    "    print(feature, sorted_correlations_major[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_minor[:20]):\n",
    "    print(feature, sorted_correlations_minor[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_ids = list()\n",
    "for idx, document in tqdm(enumerate(documents)):\n",
    "    flag = 0\n",
    "    for token in nlp_light(document):\n",
    "        if(token.text == 'co2-sensing'):\n",
    "            flag = 1\n",
    "            break\n",
    "    if(flag == 1):\n",
    "        marked_ids.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for idx in marked_ids:\n",
    "    for token in nlp_light(documents[idx]):\n",
    "        if(token.text == 'co2-sensing'):\n",
    "            counter[idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[3638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(boolean_accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(int_accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(correlations_accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_decisions = list()\n",
    "selected_reviews = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = ['Reject', 'Accept', 'Minor Revision', 'Major Revision']\n",
    "decision = random.choice(decisions)\n",
    "selected_decisions.append(decision)\n",
    "review = np.random.choice(data_by_revision['0'][decision])\n",
    "selected_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind=9\n",
    "print(selected_decisions[ind])\n",
    "selected_reviews[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_decisions[10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = dict()\n",
    "all_metrics['NB'] = dict()\n",
    "all_metrics['LR'] = dict()\n",
    "all_metrics['RF'] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics['NB']['WC_TT'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='NB', features='count', \n",
    "                                      normalize=True, binary=True)\n",
    "    all_metrics['NB']['WC_TT'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['NB']['WC_TF'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='NB', features='count', \n",
    "                                      normalize=True, binary=False)\n",
    "    all_metrics['NB']['WC_TF'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['NB']['WC_FF'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='NB', features='count', \n",
    "                                      normalize=False, binary=False)\n",
    "    all_metrics['NB']['WC_TT'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['LR']['WC_TT'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR', features='count', \n",
    "                                      normalize=True, binary=True)\n",
    "    all_metrics['LR']['WC_TT'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a5eb3de0493d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     metrics = clf_utils.cross_validate(documents, labels, clf_type='LR', features='count', \n\u001b[0;32m----> 4\u001b[0;31m                                       normalize=True, binary=False)\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WC_TF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/Thesis/scripts/classifier_utils.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(self, documents, labels, clf_type, multi_class, features, normalize, binary)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mall_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/Thesis/scripts/classifier_utils.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, train_docs, y_train, test_docs, y_test, clf_type, multi_class, features, normalize, binary)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'count'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__truediv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__truediv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_divide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__div__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_divide\u001b[0;34m(self, other, true_divide, rdivide)\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrdivide\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrue_divide\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/warnings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, message, category, filename, lineno, file, line, source)\u001b[0m\n\u001b[1;32m    396\u001b[0m                         \"line\", \"source\")\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     def __init__(self, message, category, filename, lineno, file=None,\n\u001b[0m\u001b[1;32m    399\u001b[0m                  line=None, source=None):\n\u001b[1;32m    400\u001b[0m         \u001b[0mlocal_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_metrics['LR']['WC_TF'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR', features='count', \n",
    "                                      normalize=True, binary=False)\n",
    "    all_metrics['LR']['WC_TF'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = clf_utils.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method ClassifierUtils.tokenizer of <classifier_utils.ClassifierUtils object at 0x1a2e9860b8>>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sohamp/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py:597: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.true_divide(self.todense(), other)\n"
     ]
    }
   ],
   "source": [
    "sum_ = np.sum(x_train, axis=1)\n",
    "x_train = x_train/sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nan_positions = np.argwhere(np.isnan(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206330, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(nan_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4210,      0],\n",
       "       [  4210,      1],\n",
       "       [  4210,      2],\n",
       "       ...,\n",
       "       [ 10313, 103162],\n",
       "       [ 10313, 103163],\n",
       "       [ 10313, 103164]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[10313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
