{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from classifier_utils import ClassifierUtils\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp_light = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "alphabet = string.ascii_lowercase\n",
    "clf_utils = ClassifierUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/review_decisions.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_revision = dict()\n",
    "for doc_id in data:\n",
    "    if('revisions') in data[doc_id]:\n",
    "        for revision in data[doc_id]['revisions']:\n",
    "            if(revision not in data_by_revision):\n",
    "                data_by_revision[revision] = dict()\n",
    "            if('reviews' in data[doc_id]['revisions'][revision]):\n",
    "                for review in data[doc_id]['revisions'][revision]['reviews']:\n",
    "                    review_obj = data[doc_id]['revisions'][revision]['reviews'][review]\n",
    "                    if('text' in review_obj and 'decision' in review_obj):\n",
    "                        if(len(review_obj['text'].strip()) > 0 and len(review_obj['decision']) > 0):\n",
    "                            if(review_obj['decision'] not in data_by_revision[revision]):\n",
    "                                data_by_revision[revision][review_obj['decision']] = list()\n",
    "                            data_by_revision[revision][review_obj['decision']].append(review_obj['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data_by_revision['0']['Reject'] + data_by_revision['0']['Accept'] + data_by_revision['0']['Major Revision'] + data_by_revision['0']['Minor Revision']\n",
    "labels = [0]*len(data_by_revision['0']['Reject']) + [1]*len(data_by_revision['0']['Accept']) + [2]*len(data_by_revision['0']['Major Revision']) + [3]*len(data_by_revision['0']['Minor Revision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091dd46b62104cc4b5539fd5dafa5dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15434), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_word_counter = dict()\n",
    "document_word_counter[0] = Counter()\n",
    "document_word_counter[1] = Counter()\n",
    "document_word_counter[2] = Counter()\n",
    "document_word_counter[3] = Counter()\n",
    "for idx, document in enumerate(tqdm(documents)):\n",
    "    for token in nlp_light(document):\n",
    "        document_word_counter[labels[idx]][token.text.lower()] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_nb = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels)\n",
    "    all_metrics_nb.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_features = clf_utils.get_nb_features(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[0][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[1][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[2][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[3][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_lr_ovr = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR')\n",
    "    all_metrics_lr_ovr.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are features from One vs Rest\n",
    "\"\"\"\n",
    "ordered_features = clf_utils.get_lr_features(documents, labels, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[0][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[1][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[2][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in ordered_features[3][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_lr_mn = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR', multi_class='multinomial')\n",
    "    all_metrics_lr_mn.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are features from Multinomial Logistic Regression\n",
    "\"\"\"\n",
    "ordered_features = clf_utils.get_lr_features(documents, labels, multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[0][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[1][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[2][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[3][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = dict()\n",
    "wordcloud['Reject'] = WordCloud().generate(' '.join(data_by_revision['0']['Reject']))\n",
    "wordcloud['Accept'] = WordCloud().generate(' '.join(data_by_revision['0']['Accept']))\n",
    "wordcloud['Major Revision'] = WordCloud().generate(' '.join(data_by_revision['0']['Major Revision']))\n",
    "wordcloud['Minor Revision'] = WordCloud().generate(' '.join(data_by_revision['0']['Minor Revision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Reject'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Accept'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Major Revision'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Minor Revision'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=clf_utils.tokenizer)\n",
    "count_vectorizer.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_features = count_vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boolean_reject = np.array(labels) == np.array([0]*len(labels))\n",
    "int_reject = np.array([int(value) for value in boolean_reject])\n",
    "boolean_accept = np.array(labels) == np.array([1]*len(labels))\n",
    "int_accept = np.array([int(value) for value in boolean_accept])\n",
    "boolean_major = np.array(labels) == np.array([2]*len(labels))\n",
    "int_major = np.array([int(value) for value in boolean_major])\n",
    "boolean_minor = np.array(labels) == np.array([3]*len(labels))\n",
    "int_minor = np.array([int(value) for value in boolean_minor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()\n",
    "correlations_reject = [np.corrcoef(document_features[:,idx], int_reject)[0][1] for idx in range(len(feature_names))]\n",
    "correlations_accept = [np.corrcoef(document_features[:,idx], int_accept)[0][1] for idx in range(len(feature_names))]\n",
    "correlations_major = [np.corrcoef(document_features[:,idx], int_major)[0][1] for idx in range(len(feature_names))]\n",
    "correlations_minor = [np.corrcoef(document_features[:,idx], int_minor)[0][1] for idx in range(len(feature_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_names_reject = [feature_names[idx] for idx in np.argsort(correlations_reject)][::-1]\n",
    "sorted_feature_names_accept = [feature_names[idx] for idx in np.argsort(correlations_accept)][::-1]\n",
    "sorted_feature_names_major = [feature_names[idx] for idx in np.argsort(correlations_major)][::-1]\n",
    "sorted_feature_names_minor = [feature_names[idx] for idx in np.argsort(correlations_minor)][::-1]\n",
    "sorted_correlations_reject = np.sort(correlations_reject)[::-1]\n",
    "sorted_correlations_accept = np.sort(correlations_accept)[::-1]\n",
    "sorted_correlations_major = np.sort(correlations_major)[::-1]\n",
    "sorted_correlations_minor = np.sort(correlations_minor)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_reject[:20]):\n",
    "    print(feature, sorted_correlations_reject[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_accept[:20]):\n",
    "    print(feature, sorted_correlations_accept[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_major[:20]):\n",
    "    print(feature, sorted_correlations_major[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_minor[:20]):\n",
    "    print(feature, sorted_correlations_minor[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_ids = list()\n",
    "for idx, document in tqdm(enumerate(documents)):\n",
    "    flag = 0\n",
    "    for token in nlp_light(document):\n",
    "        if(token.text == 'co2-sensing'):\n",
    "            flag = 1\n",
    "            break\n",
    "    if(flag == 1):\n",
    "        marked_ids.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for idx in marked_ids:\n",
    "    for token in nlp_light(documents[idx]):\n",
    "        if(token.text == 'co2-sensing'):\n",
    "            counter[idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[3638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(boolean_accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(int_accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(correlations_accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_decisions = list()\n",
    "selected_reviews = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = ['Reject', 'Accept', 'Minor Revision', 'Major Revision']\n",
    "decision = random.choice(decisions)\n",
    "selected_decisions.append(decision)\n",
    "review = np.random.choice(data_by_revision['0'][decision])\n",
    "selected_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind=9\n",
    "print(selected_decisions[ind])\n",
    "selected_reviews[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_decisions[10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = dict()\n",
    "all_metrics['NB'] = dict()\n",
    "all_metrics['LR'] = dict()\n",
    "all_metrics['RF'] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 20%|██        | 1/5 [03:21<13:25, 201.41s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [06:38<10:00, 200.25s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [09:48<06:34, 197.13s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [12:58<03:14, 194.92s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [16:10<00:00, 193.87s/it]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 20%|██        | 1/5 [03:12<12:48, 192.07s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [06:25<09:37, 192.52s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [09:37<06:24, 192.30s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [12:53<03:13, 193.41s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [16:11<00:00, 194.69s/it]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 20%|██        | 1/5 [03:22<13:31, 202.97s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [06:42<10:05, 201.93s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [10:09<06:46, 203.33s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [13:32<03:23, 203.43s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [16:55<00:00, 203.35s/it]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 20%|██        | 1/5 [03:23<13:35, 203.90s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [06:48<10:12, 204.02s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [10:04<06:43, 201.74s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [13:23<03:20, 201.00s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [16:39<00:00, 199.44s/it]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 20%|██        | 1/5 [03:14<12:56, 194.16s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [06:27<09:41, 193.91s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [09:33<06:23, 191.54s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [12:19<03:03, 183.96s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [14:46<00:00, 172.87s/it]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:44<10:58, 164.67s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [05:20<08:06, 162.16s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [07:45<05:13, 156.93s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [10:08<02:32, 152.72s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [12:36<00:00, 151.23s/it]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 20%|██        | 1/5 [02:25<09:41, 145.37s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [04:52<07:17, 145.98s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [07:22<04:54, 147.08s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [09:26<02:20, 140.33s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [11:27<00:00, 134.40s/it]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 20%|██        | 1/5 [02:02<08:08, 122.07s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [04:05<06:07, 122.51s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [06:09<04:05, 122.79s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [08:14<02:03, 123.55s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [10:21<00:00, 124.51s/it]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 20%|██        | 1/5 [02:08<08:33, 128.28s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [04:36<06:42, 134.16s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [06:58<04:33, 136.75s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [09:10<02:15, 135.12s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [11:21<00:00, 134.06s/it]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 20%|██        | 1/5 [02:09<08:39, 129.77s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 40%|████      | 2/5 [04:20<06:30, 130.05s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 60%|██████    | 3/5 [06:32<04:21, 130.68s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      " 80%|████████  | 4/5 [08:39<02:09, 129.56s/it]\u001b[A/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "100%|██████████| 5/5 [11:05<00:00, 134.35s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "all_metrics['NB']['WC_TF'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='NB', features='count', \n",
    "                                      normalize=True, binary=False)\n",
    "    all_metrics['NB']['WC_TF'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Accept\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Major Revision\n",
      "0.40683182522152467\n",
      "1.0\n",
      "0.5783164964484655\n",
      "Minor Revision\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics['NB']['WC_TF'][idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15434"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=clf_utils.tokenizer)\n",
    "clf = RandomForestClassifier(n_estimators=20, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training vectorizer\n",
      "Extracting features\n",
      "Fitting Model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training vectorizer\")\n",
    "vectorizer.fit(documents)\n",
    "print(\"Extracting features\")\n",
    "X_train = vectorizer.transform(documents)\n",
    "print(\"Fitting Model\")\n",
    "clf.fit(X_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = clf.estimators_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7804],\n",
       "       [83408],\n",
       "       [86558]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(estimator.feature_importances_ > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'addition'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[7804]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'same'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[83408]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simply'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[86558]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d21f3c7eada496c915186b7edb5915e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=103165), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_features = list()\n",
    "feature_importances = copy.deepcopy(clf.feature_importances_)\n",
    "for ind in tqdm(np.argsort(clf.feature_importances_)):\n",
    "    if(feature_importances[ind] > 0):\n",
    "        sorted_features.append(feature_names[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['must',\n",
       " 'claimed',\n",
       " 'calculated',\n",
       " 'present',\n",
       " 'simply',\n",
       " 'reasoning',\n",
       " 'sfg',\n",
       " 'table',\n",
       " 'cl-',\n",
       " 'furthermore',\n",
       " 'excursion',\n",
       " 'seem',\n",
       " 'cautious',\n",
       " 'reversible',\n",
       " 'since',\n",
       " 'difficulty',\n",
       " 'movement',\n",
       " 'sound',\n",
       " 'revise',\n",
       " 'whereas',\n",
       " 'it',\n",
       " 'input',\n",
       " 'are',\n",
       " 'interest',\n",
       " 'blocks',\n",
       " 'explanation',\n",
       " 'literature',\n",
       " 'their',\n",
       " 'computed',\n",
       " 'against',\n",
       " 'increased',\n",
       " 'there',\n",
       " 'impaired',\n",
       " 'lacking',\n",
       " 'here',\n",
       " 'concept',\n",
       " 'according',\n",
       " 'addition',\n",
       " 'has',\n",
       " 'respect',\n",
       " 'confirm',\n",
       " 'figures',\n",
       " 'experimental',\n",
       " 'substantial',\n",
       " 'same',\n",
       " 'question',\n",
       " 'roi',\n",
       " 'introduction',\n",
       " 'perform',\n",
       " 'written',\n",
       " 'low',\n",
       " 'with',\n",
       " 'main',\n",
       " 'needs',\n",
       " 'data',\n",
       " 'given',\n",
       " 'which',\n",
       " 'study',\n",
       " 'this',\n",
       " 'more']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for feature in sorted_features:\n",
    "    print(feature, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
