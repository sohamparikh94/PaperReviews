{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from classifier_utils import ClassifierUtils\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp_light = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "alphabet = string.ascii_lowercase\n",
    "clf_utils = ClassifierUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/review_decisions.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_revision = dict()\n",
    "for doc_id in data:\n",
    "    if('revisions') in data[doc_id]:\n",
    "        for revision in data[doc_id]['revisions']:\n",
    "            if(revision not in data_by_revision):\n",
    "                data_by_revision[revision] = dict()\n",
    "            if('reviews' in data[doc_id]['revisions'][revision]):\n",
    "                for review in data[doc_id]['revisions'][revision]['reviews']:\n",
    "                    review_obj = data[doc_id]['revisions'][revision]['reviews'][review]\n",
    "                    if('text' in review_obj and 'decision' in review_obj):\n",
    "                        if(len(review_obj['text']) > 0 and len(review_obj['decision']) > 0):\n",
    "                            if(review_obj['decision'] not in data_by_revision[revision]):\n",
    "                                data_by_revision[revision][review_obj['decision']] = list()\n",
    "                            data_by_revision[revision][review_obj['decision']].append(review_obj['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data_by_revision['0']['Reject'] + data_by_revision['0']['Accept'] + data_by_revision['0']['Major Revision'] + data_by_revision['0']['Minor Revision']\n",
    "labels = [0]*len(data_by_revision['0']['Reject']) + [1]*len(data_by_revision['0']['Accept']) + [2]*len(data_by_revision['0']['Major Revision']) + [3]*len(data_by_revision['0']['Minor Revision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_nb = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels)\n",
    "    all_metrics_nb.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_nb[idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_features = clf_utils.get_nb_features(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[0][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[1][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[2][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[3][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_lr_ovr = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR')\n",
    "    all_metrics_lr_ovr.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_ovr[idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are features from One vs Rest\n",
    "\"\"\"\n",
    "ordered_features = clf_utils.get_lr_features(documents, labels, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[0][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[1][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[2][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in ordered_features[3][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metrics_lr_mn = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR', multi_class='multinomial')\n",
    "    all_metrics_lr_mn.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reject\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['0']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['0']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['0']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Accept\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['1']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['1']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['1']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Major Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['2']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['2']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['2']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(\"Minor Revision\")\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['3']['precision'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['3']['recall'] for idx in range(5)]) for idx2 in range(10)]))\n",
    "print(np.mean([np.mean([all_metrics_lr_mn[idx2][idx]['3']['f1-score'] for idx in range(5)]) for idx2 in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are features from Multinomial Logistic Regression\n",
    "\"\"\"\n",
    "ordered_features = clf_utils.get_lr_features(documents, labels, multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[0][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[1][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[2][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ordered_features[3][:20]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = dict()\n",
    "wordcloud['Reject'] = WordCloud().generate(' '.join(data_by_revision['0']['Reject']))\n",
    "wordcloud['Accept'] = WordCloud().generate(' '.join(data_by_revision['0']['Accept']))\n",
    "wordcloud['Major Revision'] = WordCloud().generate(' '.join(data_by_revision['0']['Major Revision']))\n",
    "wordcloud['Minor Revision'] = WordCloud().generate(' '.join(data_by_revision['0']['Minor Revision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Reject'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Accept'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Major Revision'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud['Minor Revision'], interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=clf_utils.tokenizer)\n",
    "count_vectorizer.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_features = count_vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boolean_reject = np.array(labels) == np.array([0]*len(labels))\n",
    "int_reject = np.array([int(value) for value in boolean_reject])\n",
    "boolean_accept = np.array(labels) == np.array([1]*len(labels))\n",
    "int_accept = np.array([int(value) for value in boolean_accept])\n",
    "boolean_major = np.array(labels) == np.array([2]*len(labels))\n",
    "int_major = np.array([int(value) for value in boolean_major])\n",
    "boolean_minor = np.array(labels) == np.array([3]*len(labels))\n",
    "int_minor = np.array([int(value) for value in boolean_minor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()\n",
    "correlations_reject = [np.corrcoef(document_features[:,idx], int_reject)[0][1] for idx in range(len(feature_names))]\n",
    "correlations_accept = [np.corrcoef(document_features[:,idx], int_accept)[0][1] for idx in range(len(feature_names))]\n",
    "correlations_major = [np.corrcoef(document_features[:,idx], int_major)[0][1] for idx in range(len(feature_names))]\n",
    "correlations_minor = [np.corrcoef(document_features[:,idx], int_minor)[0][1] for idx in range(len(feature_names))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_names_reject = [feature_names[idx] for idx in np.argsort(correlations_reject)][::-1]\n",
    "sorted_feature_names_accept = [feature_names[idx] for idx in np.argsort(correlations_accept)][::-1]\n",
    "sorted_feature_names_major = [feature_names[idx] for idx in np.argsort(correlations_major)][::-1]\n",
    "sorted_feature_names_minor = [feature_names[idx] for idx in np.argsort(correlations_minor)][::-1]\n",
    "sorted_correlations_reject = np.sort(correlations_reject)[::-1]\n",
    "sorted_correlations_accept = np.sort(correlations_accept)[::-1]\n",
    "sorted_correlations_major = np.sort(correlations_major)[::-1]\n",
    "sorted_correlations_minor = np.sort(correlations_minor)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_reject[:20]):\n",
    "    print(feature, sorted_correlations_reject[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_accept[:20]):\n",
    "    print(feature, sorted_correlations_accept[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_major[:20]):\n",
    "    print(feature, sorted_correlations_major[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, feature in enumerate(sorted_feature_names_minor[:20]):\n",
    "    print(feature, sorted_correlations_minor[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_ids = list()\n",
    "for idx, document in tqdm(enumerate(documents)):\n",
    "    flag = 0\n",
    "    for token in nlp_light(document):\n",
    "        if(token.text == 'co2-sensing'):\n",
    "            flag = 1\n",
    "            break\n",
    "    if(flag == 1):\n",
    "        marked_ids.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for idx in marked_ids:\n",
    "    for token in nlp_light(documents[idx]):\n",
    "        if(token.text == 'co2-sensing'):\n",
    "            counter[idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[3638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(boolean_accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(int_accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(correlations_accept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_decisions = list()\n",
    "selected_reviews = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = ['Reject', 'Accept', 'Minor Revision', 'Major Revision']\n",
    "decision = random.choice(decisions)\n",
    "selected_decisions.append(decision)\n",
    "review = np.random.choice(data_by_revision['0'][decision])\n",
    "selected_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind=9\n",
    "print(selected_decisions[ind])\n",
    "selected_reviews[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_decisions[10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = dict()\n",
    "all_metrics['NB'] = dict()\n",
    "all_metrics['LR'] = dict()\n",
    "all_metrics['RF'] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/Users/sohamp/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      " 20%|██        | 1/5 [01:22<05:31, 82.96s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-02516c2c83f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     metrics = clf_utils.cross_validate(documents, labels, clf_type='NB', features='count', \n\u001b[0;32m----> 4\u001b[0;31m                                       normalize=True, binary=True)\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mall_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NB'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WC_TT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/Thesis/scripts/classifier_utils.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(self, documents, labels, clf_type, multi_class, features, normalize, binary)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mall_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mall_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/Thesis/scripts/classifier_utils.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, train_docs, y_train, test_docs, y_test, clf_type, multi_class, features, normalize, binary)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# vectorizer = CountVectorizer(tokenizer=self.tokenizer, min_df=0.1, max_df=0.9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'count'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    327\u001b[0m                                                tokenize)\n\u001b[1;32m    328\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 329\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/Thesis/scripts/classifier_utils.py\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_light\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforbidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer_sw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             )\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_docs_and_golds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/spacy/lang/lex_attrs.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_metrics['NB']['WC_TT'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='NB', features='count', \n",
    "                                      normalize=True, binary=True)\n",
    "    all_metrics['NB']['WC_TT'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['NB']['WC_TF'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='NB', features='count', \n",
    "                                      normalize=True, binary=False)\n",
    "    all_metrics['NB']['WC_TF'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['NB']['WC_FF'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='NB', features='count', \n",
    "                                      normalize=False, binary=False)\n",
    "    all_metrics['NB']['WC_TT'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['LR']['WC_TT'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR', features='count', \n",
    "                                      normalize=True, binary=True)\n",
    "    all_metrics['LR']['WC_TT'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['LR']['WC_TF'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR', features='count', \n",
    "                                      normalize=True, binary=False)\n",
    "    all_metrics['LR']['WC_TF'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['LR']['WC_FT'] = list()\n",
    "for _ in range(10):\n",
    "    metrics = clf_utils.cross_validate(documents, labels, clf_type='LR', features='count', \n",
    "                                      normalize=False, binary=False)\n",
    "    all_metrics['LR']['WC_FT'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
